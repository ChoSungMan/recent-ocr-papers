\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\date{}

\begin{document}

\section{FOTS: Fast Oriented Text Spotting with a Unified
Network}\label{header-n2}

Xuebo Liu, Ding Liang, Shi Yan, Dagui Chen, Yu Qiao, and Junjie Yan

SenseTime Group Ltd. Shenzhen Institutes of Advanced Technology, Chinese
Academy of Sciences.

\subsection{Abstract}\label{header-n5}

\begin{itemize}
\item
  Most existing methods treat text detection and recognition as separate
  tasks.
\item
  end-to-end trainable FOTS network for simultaneous detection and
  recognition, \underline{sharing computation and visual inforamtion
  among the two complemntary tasks.}
\item
  \underline{RoIRotate is introduced} to share convolutional features
  between detection and recognition.
\item
  state-of-the-art methods. ICDAR 2015, ICDAR 2018 MLT and ICDAR 2013
\end{itemize}

\subsection{Introduction}\label{header-n16}

\begin{itemize}
\item
  The most common way in scene text readings is to divide it into text
  detection and text recognition, which are handled as two separate
  tasks.
\item
  In text detection, usually a convolutional neural network is used to
  extract feature maps from a scene image, and then different decoders
  are used to decode the regions.
\item
  Text recognition, a network for \underline{sequential prediction is
  conducted on top of text regions, one by one. It leads to heavy time
  cost} especially for images with a number of text regions.
\item
  seperate architecture ignores the correlation in visual cues shared in
  detection and recognition.
\item
  \underline{The key to connec detection and recognition is the
  ROIRotate}, which gets proper features from feature maps according to
  the oriented detection bounding boxes.
\item
  The fully convolutional network based oriendted text detection branch
  is built on top of the feature map to predict the detection boxes.
\item
  The RoIRotate operator extracts text proposal features corresponding
  to the detection results from the feature map. 
\item
  The text proposal features are then fed into RNN encoder and CTC
  decoder for text recognition.
\item
  FOTS is the first end-to-end trainable framework for oriented text
  detection and recognition.
\end{itemize}

\subsection{Methodology}\label{header-n37}

\begin{itemize}
\item
  \textbf{Overall Architecture}

  \begin{figure}
  \centering
  \includegraphics{/Users/sungmancho/Projects/001.lab/002.ocr/paper_review/images/FOTS/architecture.png}
  \caption{}
  \end{figure}

  \begin{itemize}
  \item
    The \underline{backbone of the shared network is ResNet-50}.
    Inspired by FPN, concatenate low-level feature maps and high-level
    semantic feature maps.
  \item
    The \underline{text detection branch} outputs dense per-pixel
    prediction of text using features produced by shared convolutions.
  \item
    With oriented text region proposals produced by detection branch,
    the proposed \underline{RoIRotate} converts corresponding shared
    features into fixed-height representations while keeping the
    original region aspect ratio.
  \item
    Finally, the \underline{text recognition branch} recognizes words in
    region proposals. CNN and LSTM are adopted to encode text sequence
    information, followd by a CTC decoder.
  \item
    \emph{Shared conv net architecture}
  \end{itemize}
\end{itemize}

\begin{figure}
\centering
\includegraphics{/Users/sungmancho/Projects/001.lab/002.ocr/paper_review/images/FOTS/shared_conv.png}
\caption{}
\end{figure}

\begin{itemize}
\item
  \textbf{Text Detection Branch}

  \begin{itemize}
  \item
    As there are a lot of small text boxes in natural scene images, we
    upscale the feautre maps from 1/32 to 1/4 size of the original input
    image in shared convolutions.
  \item
    After extracting shared features, one convolution is applied to
    output dense per-pixel predictions of words. 
  \item
    The first channel computes the probability of each pixel being a
    positive sample. Similar to EAST, pixels in shrunk version of the
    original text regions are considered positive. For each positive
    sample, the following 4 channels predict its distances to top,
    bottom, left, right sides of the bounding box that contains this
    pixel, and the last channel predict the orientation of the related
    bounding box. 
  \item
    Final detection results are produced by \underline{applying
    thresholding and NMS} to these positive samples.
  \item
    In our experiments, we observe that many patterns similar to text
    stroke are hard to classify, such as fences, latices, etc. we adopt
    \underline{online hard example mining(OHEM)} to better distinguish
    these patterns.
  \item
    The \underline{detection branch loss function is composed of two
    terms}: text classification and bounding box regression. text
    classification term can be seen as pixel-wise classification loss
    for a down-sampled score map. \underline{Only shrunk version of the
    original text region is considered as the positive area}, while the
    area between the bounding box and the shrunk version is considered
    as "NOT CARE", and does not contribute to the loss for the
    classification.
  \item
    Denote the set of selected positive elements by OHEM in the score
    map as:

    \[{ L }_{ cls }\quad =\quad \frac { 1 }{ |\Omega | } \sum _{ x\in \Omega  }^{  }{ H({ p }_{ x },{ p }_{ x }^{ * }) }\qquad(Eq.1)\]

    \[\qquad\qquad\qquad\qquad\qquad\quad\qquad  = \quad \frac { 1 }{ |\Omega | } \sum _{ x\in \Omega  }^{  }{ (-{ p }_{ x }^{ * }log{ p }_{ x }-(1-{ p }_{ x }^{ * })log(1-{ p }_{ x })) }\qquad(Eq.2)\]

    \begin{itemize}
    \item
      where \textbar{}.\textbar{} is the number of elements in a set,
      and \(H({ p }_{ x },{ p }_{ x }^{ * }) \) represents the cross
      entropy loss between \(p_x\), the prediction of the score map, and
      \(p_x^*\), the binary label that indicates text or non-text.
    \end{itemize}
  \item
    As for the regression loss, we adopt the IoU loss and the rotation
    angle loss, since they are robust to variation in object shape,
    scale and orientation:

    \[{ L }_{ reg }\quad =\quad \frac { 1 }{ |\Omega | } \sum _{ x\in \Omega  }^{  }{IoU(R_x,R_x^*) + \lambda_\theta(1-cos(\theta_x,\theta_x^*))}\qquad(Eq.3)\]

    \begin{itemize}
    \item
      Here, \(IoU(R_x,R_x^*)\) is the IoU loss between the predicted
      bounding box \(R_x\), and the ground truth \(R_x^*\). 
    \item
      Second term is rotation angle loss. We set the hyper-parameter
      \(\lambda_\theta\) to 10 in experiments.
    \end{itemize}
  \item
    Therefore the full detection loss can be written as:

    \[L_{detect} = L_{cls} +\lambda_{reg}L_{reg}\qquad(Eq.4)\]

    \begin{itemize}
    \item
      where a hyper-parameter \(\lambda_{reg}\) balacnes two losses,
      which is set to 1 in our experiments.
    \end{itemize}
  \end{itemize}
\item
  \textbf{ROIRotate}

  \begin{itemize}
  \item
    RoIRotate applies transformation on oriented feature regions to
    obtain axis-aligned feature maps.
  \end{itemize}

  \begin{figure}
  \centering
  \includegraphics{/Users/sungmancho/Projects/001.lab/002.ocr/paper_review/images/FOTS/roi_rotate.png}
  \caption{}
  \end{figure}

  \begin{itemize}
  \item
    Fix the output height and keep the aspect ratio unchanged to deal
    with the variation in text length.
  \item
    Compared to RoIPooling and RoIAlign, \underline{RoIRotate provides a
    more general operation} for extracting features for regions of
    interest. 
  \item
    We also compare to RROI pooling proposed in RRPN. RRoI pooling
    transforms the rotated region to fixed size region through
    max-pooling, while we use \textbf{bilinear interpolation} to compute
    the values of the output. \underline{This operation avoids
    misalignments between the RoI and the extracted features, and
    additionally it makes the lengths of the output features variable,
    which is more suitable for text recognition}.
  \item
    This process can be divided into two steps.

    \begin{itemize}
    \item
      First, affine transformation parameters are computed via predicted
      or ground truth coordinates of text proposals. Then, affine
      transformations are applied to shared feature maps for each region
      respectively, and canonical horizontal feature maps of text
      regions are obtained. The first step can be formulated as:

      \[t_x = l*cos\theta - t*sin\theta - x\qquad(Eq.5)\]

      \[t_y = t*cos\theta + l*sin\theta - y\qquad(Eq.6)\]

      \[s = \frac {h_t}{t+b}\qquad(Eq.7)\]

      \[w_t = s * (l + r)\qquad(Eq.8)\]

      \[\\ M\quad =\quad \begin{bmatrix} cos\theta  & -sin\theta  & 0 \\ sin\theta  & cos\theta  & 0 \\ 0 & 0 & 1 \end{bmatrix}\begin{bmatrix} s & 0 & 0 \\ 0 & s & 0 \\ 0 & 0 & 1 \end{bmatrix}\begin{bmatrix} 1 & 0 & t_ x \\ 0 & 1 & t_y \\ 0 & 0 & 1 \end{bmatrix}\qquad(Eq.9)\]

      \[= s\begin{bmatrix} cos\theta  & -sin\theta  & t_xcos\theta - t_ysin\theta \\ sin\theta  & cos\theta  & t_xsin\theta + t_ycos\theta \\ 0 & 0 & \frac1s \end{bmatrix}\qquad(Eq.10)\]
    \item
      M is the affine transformation matrix. \(h_t, w_t\) represent
      height (equals 8 in our setting) and width of feature maps after
      affine transformation. \((x,y)\) represents the coordinates of a
      point in shared feature maps and \((t, b, l, r)\) stands for
      distance to top, bottom, left, right sides of the text proposal
      respectively, and \(\theta\) for the orientation. \((t, b, l, r)\)
      and \(\theta\) can be given by ground truth or the detection
      branch.
    \item
      With the transformation parameters, it is easy to produce the
      \underline{final RoI feature} using affine transformation.

      \begin{pmatrix} x_i^s \\ y_i^s \\ 1 \end{pmatrix} \quad= \quad M^{-1} \begin{pmatrix} x_i^t \\ y_i^t \\ 1 \end{pmatrix}\qquad(Eq.11)

      and for ∀i ∈ {[}1...ht{]}, ∀j ∈ {[}1...wt{]}, ∀c ∈
      {[}1\ldots{}C{]},

      \[V^{ c }_{ ij }=\sum _{ n }^{ h_s }{ \sum _{ m }^{ w_s }{ U^{ c }_{ nm }k(x^{ s }_{ ij }-m;\Phi _{ x })k(y^{ s }_{ iJ }-n;\Phi _{ y }) }  }\qquad(Eq.12)\]

      where \(V^{ c }_{ ij }\) is the output value at location
      \((i, j)\) in channel \(c\) and \(U^{ c }_{ nm }\) is the input
      value at location \((n, m)\) in channel \(c\). \(h_s, w_s\)
      represent the height and width of the input, and
      \(\phi_x, \phi_y\) are the parameters of a generic sampling kernel
      \(k()\), which defines the interpolation methods, specifically
      bilinear interpolation in this work. As the width of text
      proposals may vary, in practice, we pad the feature maps to the
      longest width and ignore the padding parts in recognition loss
      function.
    \end{itemize}
  \item
    Different from object classification, \underline{text recognition is
    very sensitive to detection noise}. so we use ground truth text
    regions instead of predicted text regions during training. 
  \end{itemize}
\item
  \textbf{Text Recognition Branch}

  \begin{itemize}
  \item
    Considering the length of the label sequence in text regions, input
    features to LSTM are reduced only twice (to 1/4) along width axis
    through shared convolutions from the original image.
  \item
    Text recognition branch consists of VGG like sequential
    convolutions, \underline{poolings with reduction along height axis
    only}, one bi-directional LSTM, one fully-connection and the final
    CTC decoder.
  \end{itemize}

  \begin{figure}
  \centering
  \includegraphics{/Users/sungmancho/Projects/001.lab/002.ocr/paper_review/images/FOTS/recognition_branch.png}
  \caption{}
  \end{figure}

  \begin{itemize}
  \item
    First, spatial features are fed into several sequential convolutions
    and poolings along height axis with dimension reduction to extract
    higher-level features.
  \item
    Next, the extracted higher-level feature maps \(L ∈ R^{C ×H ×W}\)
    are permuted to time major form as a sequence
    \(l_1,…,l_W ∈ R^{C ×H}\) and fed into RNN for encoding. Here we use
    a bi-directional LSTM, with D=256 output channels per direction, to
    capture range dependencies of the input sequential features.
  \item
    Then, hidden states \(h_1,…,h_W ∈ R^D\) calculated at each time step
    in both directions are summed up and fed into a fully-connection,
    which gives each state its distribution \(x_t ∈ R^{|S|}\) over the
    character classes \(S\). 
  \item
    To avoid overfitting on small training datasets like ICDAR2015, we
    add dropout before fully-connection.
  \item
    Finally, CTC is used to transform frame-wise classification scores
    to label sequence. Given probability distribution \(x_t\) over \(S\)
    of each \(h_t\), and ground truth label sequence
    \(y^* = {y_1,…,y_T}, T\ll W\), the conditional probability of the
    label \(y^*\) is the sum of probabilities of all paths \(\pi\)
    agreeing with:

    \[p(y^{ * }|x)\quad =\quad \sum _{ \pi\in B^{-1}(y^*) }^{  }{p(\pi|x)  } \qquad(Eq.13)\]

    \begin{itemize}
    \item
      where \(B\) defines a many-to-one map from the set of possible
      labellings with blanks and repeated labels to \(y^*\). 
    \end{itemize}
  \item
    The training process attempts to maximize the log likelihood of
    summation of Eq.A over the whole training set. the recognition loss
    can be formulated as:

    \[L_{recog} \quad = \quad \frac 1N \sum _{ n=1 }^{N}{log p(y_n^*|x)  }\qquad(Eq.14)\]

    \begin{itemize}
    \item
      Where N is the number of text regions in an input image, and
      \(y_n^*\) is the recognition label.
    \end{itemize}
  \item
    Combined with detection loss \(L_{detect}\) in , the full multi-task
    loss function is:

    \[L = L_{detect} + \lambda_{recog}L_{recog}\qquad(Eq.15)\]

    \begin{itemize}
    \item
      where a hyper-parameter \(\lambda_{recog}\) controls the trade-off
      between two losses. \(\lambda_{recog}\) Is set to 1 in our
      experiments.
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsection{Conclusion}\label{header-n172}

\begin{itemize}
\item
  A novel RoIRotate operation is proposed to unify detection and
  recognition into an end-to-end pipeline. \underline{By sharing
  convolutional features, the text recognition step is nearly cost-free,
  which enables our system to run at real-time speed.} 
\end{itemize}

\end{document}
